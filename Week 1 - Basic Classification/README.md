# Week 1: ML Fundamentals and Basic Classification

## Path 1: Minimum

### Part 1: ML Fundamentals

1. [Can I learn ML theory without math?](https://machinelearningmastery.com/youre-wrong-machine-learning-not-hard/)

1. [Introduction to ML](https://www.youtube.com/watch?v=Gv9_4yMHFhI)

1. [Slides used in our intro to ML workshop](https://github.com/majimearun/crux-ml-workshop/blob/main/Week%201%20-%20Basic%20Classification/Intro%20to%20ML%20slides.pdf) (attached): For those of you who weren't able to attend the offline introduction, see what you can understand from the slides and skip the rest. Once you are done with this weeks content, come back to it and see if you can make sense out of it.

1. [Understanding the concept of train-test split](https://www.youtube.com/watch?v=_vdMKioCXqQ)

1. [How do you split your dataset though?](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)

1. [Points to keep in mind while splitting data](https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50)

1. [What is overfitting?](https://machinelearningmastery.com/a-simple-intuition-for-overfitting/)

1. [Basic intuition for feature scaling part 1](https://www.geeksforgeeks.org/ml-feature-scaling-part-1/)

1. [Basic intuition for feature scaling part 2](https://www.geeksforgeeks.org/ml-feature-scaling-part-2/)

1. [Understanding the feature engineering process](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)

1. [Feature engineering kaggle tutorial](https://www.kaggle.com/learn/feature-engineering)

1. [Preprocessing cheatsheet](https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9)

1. [How to define your machine learning problem](https://machinelearningmastery.com/how-to-define-your-machine-learning-problem/)

### Part 2: Basic Classification

1. [Classification vs regression](https://www.youtube.com/watch?v=TJveOYsK6MY)

1. [The confusion matrix](https://www.youtube.com/watch?v=Kdsp6soqA7o)

1. Your first classification model: [Naive Bayes](https://www.youtube.com/watch?v=O2L2Uv9pdDA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=45)

1. [Implementing Naive Bayes using sklearn](https://www.datacamp.com/tutorial/naive-bayes-scikit-learn)

1. Second classification model: [Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=19)

1. [Implementing Logistic Regression using sklearn](https://www.datacamp.com/tutorial/understanding-logistic-regression-python)

## Path 2: Additional

### Part 3: Some extra stuff for the curious

#### ML Fundamentals (Extra)

1. [Introduction to Machine Learning by MIT OpenCourseWare](https://www.youtube.com/watch?v=h0e2HAPTGF4)

1. [Some extra methods for encoding data](https://www.youtube.com/watch?v=589nCGeWG1w&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=52)

1. [Example usage of a pipeline for preprocessing](https://www.youtube.com/watch?v=irHhDMbw3xo&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&index=11)

1. [Reference for basic feature engineering](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods/notebook#6.-Discretization-)

1. [Example of manual feature engineering](https://www.kaggle.com/code/willkoehrsen/introduction-to-manual-feature-engineering#Introduction:-Manual-Feature-Engineering)

1. [Advanced feature engineering](https://www.kaggle.com/code/seneralkan/advanced-feature-engineering#5.-Feature-Extraction)

1. [Preprocessing tips using scikitlearn](https://www.youtube.com/playlist?list=PL5-da3qGB5ID7YYAqireYEew2mWVvgmj6)

#### Basic Classification (Extra)

1. [Sensitivity and specificity](https://www.youtube.com/watch?v=vP06aMoz4v8)

1. [Logistic regression example 2](https://towardsdatascience.com/logistic-regression-using-python-sklearn-numpy-mnist-handwriting-recognition-matplotlib-a6b31e2b166a)

1. [Logisitic regression math part 1](https://www.youtube.com/watch?v=vN5cNN2-HWE&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=20): Coefficients

1. [Logisitc regression math part 2](https://www.youtube.com/watch?v=BfKanl1aSG0&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=21): Maximum Likelihood

1. [Logisitc regression math part 3](https://www.youtube.com/watch?v=xxFYro8QuXA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=22): R2 and p values

1. [Naive bayes sklearn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)

1. [Logistic regression sklearn documentation](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
